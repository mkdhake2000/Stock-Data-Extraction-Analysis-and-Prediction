{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from datetime import date\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting webdriver connection in Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver1 = webdriver.Chrome(executable_path = \"chromedriver.exe\")\n",
    "driver1.set_window_size(720,1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class with methods to create dataframe of data gathered from site and to remove improper data and converting to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrapedata:\n",
    "    #Method to create dataframe of data gathered from site\n",
    "    def datastock(st_day,st_month,st_year,com_name):\n",
    "        #Getting today's date until which to find the data\n",
    "        today = date.today()\n",
    "\n",
    "        #Ensuring that you get the correct date\n",
    "        # while True:\n",
    "        #     if(st_day in range(1,32)):\n",
    "        #         break\n",
    "        #     st_day = int(input(\"Enter starting date(correct day): \"))\n",
    "        # while True:\n",
    "        #     if(st_month in range(1,13)):\n",
    "        #         break\n",
    "        #     st_month = int(input(\"Enter starting date(correct month): \"))\n",
    "        # while True:\n",
    "        #     if(st_year in range(2008,today.year)):\n",
    "        #         break\n",
    "        #     st_year = int(input(\"Enter starting date(correct year): \"))\n",
    "\n",
    "        #Getting the lowest date you can enter\n",
    "        d0 = date(2008,1,1)\n",
    "        #Initial date code for getting url\n",
    "        init_period = 1199145600\n",
    "        #amount to be added for getting next day code\n",
    "        day_gap = 86400\n",
    "\n",
    "        #Getting the code for starting date\n",
    "        d1 = date(st_year, st_month, st_day)\n",
    "        delta1 = d1 - d0\n",
    "        period1 = init_period + delta1.days * day_gap\n",
    "\n",
    "        #Getting the code for ending date\n",
    "        d2 = date(today.year, today.month, today.day)\n",
    "        delta2 = d2 - d0\n",
    "        period2 = init_period + delta2.days * day_gap\n",
    "\n",
    "        #To get the number of days in this period\n",
    "        delta = d2 - d1\n",
    "\n",
    "        #com_name = input(\"Enter Name of Stock(with NS/BO): \")\n",
    "\n",
    "        #driver1 = webdriver.Chrome(executable_path = \"chromedriver.exe\")\n",
    "        #driver1.set_window_size(720,1280)\n",
    "\n",
    "        #Opening the link\n",
    "        driver1.get(\"https://finance.yahoo.com/quote/\"+com_name+\"/history?period1=\"+str(period1)+\"&period2=\"+str(period2)+\"&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true\")\n",
    "\n",
    "        #To scroll to end of the web page\n",
    "        for k in range(0,delta.days, 100):\n",
    "            try :\n",
    "                ActionChains(driver1).move_to_element(driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(k)+\"]\")).perform()\n",
    "                continue\n",
    "            except :\n",
    "                driver1.execute_script(\"window.scrollTo(0, 200000);\")\n",
    "\n",
    "        #Getting the number of days for which data is there in the site\n",
    "        a = driver1.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div').find_elements_by_tag_name('tr')\n",
    "        a = a[1:-1]\n",
    "        \n",
    "        #Creating empty lists\n",
    "        s_date = []\n",
    "        s_open = []\n",
    "        s_high = []\n",
    "        s_low = []\n",
    "        s_close = []\n",
    "        s_adjclose = []\n",
    "        s_vol = []\n",
    "\n",
    "        #Getting the data in lists \n",
    "        for i in range(1, len(a)+1):\n",
    "            try:\n",
    "                diff = driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[2]\").text\n",
    "                if(diff.split(\" \")[1] == \"Dividend\" or diff.split(\" \")[1] == \"Stock\" or diff == \"-\"):\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            s_date.append(driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[1]\").text)\n",
    "            s_open.append((driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[2]\").text).replace(\",\",\"\"))\n",
    "            s_high.append((driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[3]\").text).replace(\",\",\"\"))\n",
    "            s_low.append((driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[4]\").text).replace(\",\",\"\"))\n",
    "            s_close.append((driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[5]\").text).replace(\",\",\"\"))\n",
    "            s_adjclose.append((driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[6]\").text).replace(\",\",\"\"))\n",
    "            s_vol.append((driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[7]\").text).replace(\",\",\"\"))\n",
    "        \n",
    "        #Reversing the lists to get dates in ascending order\n",
    "        s_date.reverse()\n",
    "        s_high.reverse()\n",
    "        s_low.reverse()\n",
    "        s_open.reverse()\n",
    "        s_close.reverse()\n",
    "        s_vol.reverse()\n",
    "        s_adjclose.reverse()\n",
    "\n",
    "        #Creating dictionary for these lists\n",
    "        stock_data = {'Date' :s_date, 'High':s_high, 'Low': s_low,'Open': s_open,'Close': s_close, 'Volume': s_vol, 'Adj Close': s_adjclose}\n",
    "        #Creating dataframe from dictionary\n",
    "        sd_df = pd.DataFrame(stock_data)\n",
    "        \n",
    "        #Returning dataframe by removing improper values and converting to numeric values\n",
    "        return Scrapedata.remove_improper_values_and_convert_to_numeric(sd_df)\n",
    "        \n",
    "\n",
    "    #Method for preprocessing the data\n",
    "    def remove_improper_values_and_convert_to_numeric(a):\n",
    "        #Removing improper values\n",
    "        a = a[a['High'] != '-']\n",
    "        a = a[a['Low'] != '-']\n",
    "        a = a[a['Open'] != '-']\n",
    "        a = a[a['Close'] != '-']\n",
    "        a = a[a['Volume'] != '-']\n",
    "        a = a[a['Adj Close'] != '-']\n",
    "        #Converting to numeric values\n",
    "        a['High'] = pd.to_numeric(a['High'])\n",
    "        a['Low'] = pd.to_numeric(a['Low'])\n",
    "        a['Open'] = pd.to_numeric(a['Open'])\n",
    "        a['Close'] = pd.to_numeric(a['Close'])\n",
    "        a['Volume'] = pd.to_numeric(a['Volume'])\n",
    "        a['Adj Close'] = pd.to_numeric(a['Adj Close'])\n",
    "        #Returning processed dataframe\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists for companies' codes and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_list = ['VENKEYS.NS','TATASTLLP.NS','JKTYRE.NS','RAYMOND.NS','TATAMETALI.NS','TCS.NS','CTSH','INFY','HCLTECH.NS','WIPRO.NS','ONGC.NS','RAJESHEXPO.NS','ICICIBANK.NS','HDFCBANK.NS','LTI.NS','ASTRAL.NS','DEEPAKNTR.NS','PERSISTENT.NS','MINDTREE.NS','NAUKRI.NS','%5ENSEI']\n",
    "com_name_list = ['VENKEYS','TATASTLLP','JKTYRE','RAYMOND','TATAMETALI','TCS','COGNIZANT','INFOSYS','HCL','WIPRO','ONGC','RAJESHEXPORTS','ICICI','HDFC','LTI','ASTRAL','DEEPAKNTR','PERSISTENT','MINDTREE','NAUKRI','NIFTY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the function defined above for each company in this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(com_list)):\n",
    "    stock_data = Scrapedata.datastock(18,11,2018,com_list[i])\n",
    "    #Exportingto company-name.csv\n",
    "    stock_data.to_csv(com_name_list[i]+'.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running function for MINDTREE. Since we need for different period, we do it seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = Scrapedata.datastock(1,1,2012,\"MINDTREE.NS\")\n",
    "#Exporting to MINDTREE.csv\n",
    "stock_data.to_csv(\"MINDTREE-PREDICTION.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For NIFTY 50, We only need the \"Date\", \"Open\", \"High\", \"Low\", \"Close\" columns. So, We slightly re-define our class for specifically retrieving these columns only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrapedata_Nifty:\n",
    "    #Method to create dataframe of data gathered from site\n",
    "    def datastock(st_day,st_month,st_year,com_name):\n",
    "        #Getting today's date until which to find the data\n",
    "        today = date.today()\n",
    "\n",
    "        #Ensuring that you get the correct date\n",
    "        # while True:\n",
    "        #     if(st_day in range(1,32)):\n",
    "        #         break\n",
    "        #     st_day = int(input(\"Enter starting date(correct day): \"))\n",
    "        # while True:\n",
    "        #     if(st_month in range(1,13)):\n",
    "        #         break\n",
    "        #     st_month = int(input(\"Enter starting date(correct month): \"))\n",
    "        # while True:\n",
    "        #     if(st_year in range(2008,today.year)):\n",
    "        #         break\n",
    "        #     st_year = int(input(\"Enter starting date(correct year): \"))\n",
    "\n",
    "        #Getting the lowest date you can enter\n",
    "        d0 = date(2008,1,1)\n",
    "        #Initial date code for getting url\n",
    "        init_period = 1199145600\n",
    "        #amount to be added for getting next day code\n",
    "        day_gap = 86400\n",
    "\n",
    "        #Getting the code for starting date\n",
    "        d1 = date(st_year, st_month, st_day)\n",
    "        delta1 = d1 - d0\n",
    "        period1 = init_period + delta1.days * day_gap\n",
    "\n",
    "        #Getting the code for ending date\n",
    "        d2 = date(today.year, today.month, today.day)\n",
    "        delta2 = d2 - d0\n",
    "        period2 = init_period + delta2.days * day_gap\n",
    "\n",
    "        #To get the number of days in this period\n",
    "        delta = d2 - d1\n",
    "\n",
    "        #com_name = input(\"Enter Name of Stock(with NS/BO): \")\n",
    "\n",
    "        #driver1 = webdriver.Chrome(executable_path = \"chromedriver.exe\")\n",
    "        #driver1.set_window_size(720,1280)\n",
    "\n",
    "        #Opening the link\n",
    "        driver1.get(\"https://finance.yahoo.com/quote/\"+com_name+\"/history?period1=\"+str(period1)+\"&period2=\"+str(period2)+\"&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true\")\n",
    "\n",
    "        #To scroll to end of the web page\n",
    "        for k in range(0,delta.days, 100):\n",
    "            try :\n",
    "                ActionChains(driver1).move_to_element(driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(k)+\"]\")).perform()\n",
    "                continue\n",
    "            except :\n",
    "                driver1.execute_script(\"window.scrollTo(0, 200000);\")\n",
    "\n",
    "        #Getting the number of days for which data is there in the site\n",
    "        a = driver1.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div').find_elements_by_tag_name('tr')\n",
    "        a = a[1:-1]\n",
    "        \n",
    "        #Creating empty lists\n",
    "        s_date = []\n",
    "        s_open = []\n",
    "        s_high = []\n",
    "        s_low = []\n",
    "        s_close = []\n",
    "\n",
    "        #Getting the data in lists \n",
    "        for i in range(1, len(a)+1):\n",
    "            try:\n",
    "                diff = driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[2]\").text\n",
    "                if(diff.split(\" \")[1] == \"Dividend\" or diff.split(\" \")[1] == \"Stock\" or diff == \"-\"):\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            s_date.append(driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[1]\").text)\n",
    "            s_open.append((driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[2]\").text).replace(\",\",\"\"))\n",
    "            s_high.append((driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[3]\").text).replace(\",\",\"\"))\n",
    "            s_low.append((driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[4]\").text).replace(\",\",\"\"))\n",
    "            s_close.append((driver1.find_element_by_xpath(\"/html/body/div[1]/div/div/div[1]/div/div[3]/div[1]/div/div[2]/div/div/section/div[2]/table/tbody/tr[\"+str(i)+\"]/td[5]\").text).replace(\",\",\"\"))\n",
    "        \n",
    "        #Reversing the lists to get dates in ascending order\n",
    "        s_date.reverse()\n",
    "        s_high.reverse()\n",
    "        s_low.reverse()\n",
    "        s_open.reverse()\n",
    "        s_close.reverse()\n",
    "\n",
    "        #Creating dictionary for these lists\n",
    "        stock_data = {'Date' :s_date, 'Open':s_open, 'High': s_high,'Low': s_low,'Close': s_close}\n",
    "        #Creating dataframe from dictionary\n",
    "        sd_df = pd.DataFrame(stock_data)\n",
    "        \n",
    "        #Returning dataframe by removing improper values and converting to numeric values\n",
    "        return Scrapedata_Nifty.remove_improper_values_and_convert_to_numeric(sd_df)\n",
    "        \n",
    "\n",
    "    #Method for preprocessing the data\n",
    "    def remove_improper_values_and_convert_to_numeric(a):\n",
    "        #Removing improper values\n",
    "        a = a[a['High'] != '-']\n",
    "        a = a[a['Low'] != '-']\n",
    "        a = a[a['Open'] != '-']\n",
    "        a = a[a['Close'] != '-']\n",
    "        #Converting to numeric values\n",
    "        a['High'] = pd.to_numeric(a['High'])\n",
    "        a['Low'] = pd.to_numeric(a['Low'])\n",
    "        a['Open'] = pd.to_numeric(a['Open'])\n",
    "        a['Close'] = pd.to_numeric(a['Close'])\n",
    "        #Returning processed dataframe\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running function for NIFTY 50. Since we need for different period, we do it seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = Scrapedata_Nifty.datastock(1,1,2011,\"%5ENSEI\")\n",
    "#Exporting to NIFTY 50_Data.csv\n",
    "stock_data.to_csv(\"NIFTY 50_Data.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing webdriver connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver1.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c80d797ad0c0f99ceecb07d53ac314bc1d62fa8729ade7f8a491ed15ddf8184a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
